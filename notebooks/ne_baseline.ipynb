{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# はじめに\n",
    "このノートブックでは、拡張固有表現認識のベースラインモデルの作成を行います。\n",
    "まずはデータセットを読み込み、整形します。\n",
    "その後、ベースラインモデルを構築し、評価を行います。\n",
    "\n",
    "## データセットの読み込み\n",
    "この節では、拡張固有表現のデータセットを読み込みます。\n",
    "データセットには、毎日新聞1995に対して拡張固有表現が付与されたデータセットを用います。\n",
    "以下のコードを実行して、文字ベースのIOB2形式で読み込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cc354c9ad93a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmainichi_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../data/raw/corpora/mainichi'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_iob2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmainichi_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from entitypedia.evaluation.converter import to_iob2\n",
    "\n",
    "mainichi_dir = '../data/raw/corpora/mainichi'\n",
    "X, y = to_iob2(mainichi_dir)\n",
    "print(' '.join(X[0][:50]))\n",
    "print(' '.join(y[0][:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記に示したように読み込んだデータセットでは文字単位でラベルがついています。今回は単語単位で認識するベースラインモデルを作りたいため、単語レベルにラベルを付け直します。\n",
    "\n",
    "タスクとしては以下の通りです。\n",
    "\n",
    "* 文字のリストを結合して文字列にする\n",
    "* 文字列を形態素解析器で解析し、分かち書きする\n",
    "* 分かち書きした単語のリストに対してラベルを付け直す。\n",
    "\n",
    "まずは文字のリストを結合して文字列にします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\u3000◇国際貢献など４点、ビジョンの基本示す\\n\\u3000村山富市首相は年頭の記者会見で、「創造とやさしさの国造りのビジョン」と題する所感を発表した。今月中に首相を囲む学者グループが発表する「村山ビジョン」の基本'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [''.join(doc) for doc in X]\n",
    "docs[0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、結合した文字列に対して形態素解析を行います。\n",
    "形態素解析機にはMeCabを使用します。ついでに品詞情報も取得しておきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\u3000', '◇', '国際', '貢献', 'など', '４', '点', '、', 'ビジョン', 'の']\n",
      "['記号', '記号', '名詞', '名詞', '助詞', '名詞', '名詞', '記号', '名詞', '助詞']\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "t = MeCab.Tagger()\n",
    "\n",
    "\n",
    "def tokenize(sent):\n",
    "    tokens = []\n",
    "    t.parse('')  # for UnicodeDecodeError\n",
    "    node = t.parseToNode(sent)\n",
    "\n",
    "    while node:\n",
    "        feature = node.feature.split(',')\n",
    "        surface = node.surface    # 表層形\n",
    "        pos = feature[0]          # 品詞\n",
    "        tokens.append((surface, pos))\n",
    "        node = node.next\n",
    "\n",
    "    return tokens[1:-1]\n",
    "\n",
    "tokenized_docs = [[d[0] for d in tokenize(doc)] for doc in docs]\n",
    "poses = [[d[1] for d in tokenize(doc)] for doc in docs]\n",
    "\n",
    "print(tokenized_docs[0][:10])\n",
    "print(poses[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで分かち書きまではできました。その後が若干面倒です。ラベルを単語単位で付け直す必要があります。\n",
    "以下の手順でやってみましょう。\n",
    "\n",
    "1. 形態素を1つ取り出す\n",
    "2. 形態素を構成するラベルを文字列マッチングによって取り出す\n",
    "3. ラベルを修正する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = []\n",
    "for t_doc, doc, label in zip(tokenized_docs, docs, y):\n",
    "    i = 0\n",
    "    doc_tags = []\n",
    "    for word in t_doc:\n",
    "        j = len(word)\n",
    "        while not doc[i:].startswith(word):  # correct\n",
    "            i += 1\n",
    "        tag = label[i: i+j][0]\n",
    "        # print('{}\\t{}'.format(word, tag))\n",
    "        doc_tags.append(tag)\n",
    "        i += j\n",
    "    tags.append(doc_tags)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "対応付けができているか確認してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "　\tO\n",
      "◇\tO\n",
      "国際\tO\n",
      "貢献\tO\n",
      "など\tO\n",
      "４\tO\n",
      "点\tO\n",
      "、\tO\n",
      "ビジョン\tO\n",
      "の\tO\n",
      "基本\tO\n",
      "示す\tO\n",
      "　\tO\n",
      "村山\tB-person\n",
      "富市\tI-person\n",
      "首相\tB-position_vocation\n",
      "は\tO\n",
      "年頭\tB-date\n",
      "の\tO\n",
      "記者\tB-position_vocation\n"
     ]
    }
   ],
   "source": [
    "for word, tag in zip(tokenized_docs[0][:20], tags[0][:20]):\n",
    "    print('{}\\t{}'.format(word, tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大丈夫そうですね。では`tokenized_docs`と`tags`を`X`と`y`に代入してやりましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tokenized_docs\n",
    "y = tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上でデータの読み込みと整形は完了しました。\n",
    "次はベースラインモデルを作成します。\n",
    "\n",
    "# ベースラインモデルの作成\n",
    "本節では拡張固有表現を認識するベースラインモデルを作成します。\n",
    "現在の固有表現認識ではBi-LSTMとCRFを組み合わせたモデルがよく用いられます。しかし、今回のように認識するタグ数が多い場合、CRFを入れると計算量が非常に多くなり、現実的な時間で問題を解くことができなくなります。したがって、まずはシンプルなモデルで解いてみましょう。\n",
    "\n",
    "ここでは、まず単純な単語ベースBi-LSTMを試してみます。計算時間が多いようだったら、更に簡単なモデルを検討します。\n",
    "\n",
    "ではまずは、データセットを学習用と検証用に分割しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これでデータセットを分割できました。\n",
    "現在、データセットの中は文字列で表現されています。これではモデルにデータを与えることができないので前処理を行います。\n",
    "前処理のためのコードを定義していきましょう。\n",
    "具体的な前処理としては、以下を行います。\n",
    "\n",
    "* 単語を数字に変換\n",
    "* 系列長の統一\n",
    "\n",
    "少々長いですが以下のように定義できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.externals import joblib\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "UNK = '<UNK>'\n",
    "PAD = '<PAD>'\n",
    "\n",
    "class Preprocessor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self,\n",
    "                 lowercase=True,\n",
    "                 num_norm=True,\n",
    "                 vocab_init=None,\n",
    "                 padding=True,\n",
    "                 return_lengths=True):\n",
    "\n",
    "        self.lowercase = lowercase\n",
    "        self.num_norm = num_norm\n",
    "        self.padding = padding\n",
    "        self.return_lengths = return_lengths\n",
    "        self.vocab_word = None\n",
    "        self.vocab_tag  = None\n",
    "        self.vocab_init = vocab_init or {}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        words = {PAD: 0, UNK: 1}\n",
    "        tags  = {PAD: 0}\n",
    "\n",
    "        for w in set(itertools.chain(*X)) | set(self.vocab_init):\n",
    "            if w not in words:\n",
    "                words[w] = len(words)\n",
    "\n",
    "        for t in itertools.chain(*y):\n",
    "            if t not in tags:\n",
    "                tags[t] = len(tags)\n",
    "\n",
    "        self.vocab_word = words\n",
    "        self.vocab_tag  = tags\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"transforms input(s)\n",
    "        Args:\n",
    "            X: list of list of words\n",
    "            y: list of list of tags\n",
    "        Returns:\n",
    "            numpy array: sentences\n",
    "            numpy array: tags\n",
    "        Examples:\n",
    "            >>> X = [['President', 'Obama', 'is', 'speaking']]\n",
    "            >>> print(self.transform(X))\n",
    "            [\n",
    "                [1999, 1037, 22123, 48388],       # word ids\n",
    "            ]\n",
    "        \"\"\"\n",
    "        words = []\n",
    "        lengths = []\n",
    "        for sent in X:\n",
    "            word_ids = []\n",
    "            lengths.append(len(sent))\n",
    "            for word in sent:\n",
    "                word_ids.append(self.vocab_word.get(word, self.vocab_word[UNK]))\n",
    "\n",
    "            words.append(word_ids)\n",
    "\n",
    "        if y is not None:\n",
    "            y = [[self.vocab_tag[t] for t in sent] for sent in y]\n",
    "\n",
    "        if self.padding:\n",
    "            maxlen = max(lengths)\n",
    "            sents = pad_sequences(words, maxlen, padding='post')\n",
    "            if y is not None:\n",
    "                y = pad_sequences(y, maxlen, padding='post')\n",
    "                y = dense_to_one_hot(y, len(self.vocab_tag), nlevels=2)\n",
    "\n",
    "        else:\n",
    "            sents = words\n",
    "\n",
    "        if self.return_lengths:\n",
    "            lengths = np.asarray(lengths, dtype=np.int32)\n",
    "            lengths = lengths.reshape((lengths.shape[0], 1))\n",
    "            sents = [sents, lengths]\n",
    "\n",
    "        return (sents, y) if y is not None else sents\n",
    "\n",
    "    def inverse_transform(self, y):\n",
    "        indice_tag = {i: t for t, i in self.vocab_tag.items()}\n",
    "        return [indice_tag[y_] for y_ in y]\n",
    "\n",
    "    def vocab_size(self):\n",
    "        return len(self.vocab_word)\n",
    "\n",
    "    def tag_size(self):\n",
    "        return len(self.vocab_tag)\n",
    "\n",
    "\n",
    "def dense_to_one_hot(labels_dense, num_classes, nlevels=1):\n",
    "    \"\"\"Convert class labels from scalars to one-hot vectors.\"\"\"\n",
    "    if nlevels == 1:\n",
    "        num_labels = labels_dense.shape[0]\n",
    "        index_offset = np.arange(num_labels) * num_classes\n",
    "        labels_one_hot = np.zeros((num_labels, num_classes), dtype=np.int32)\n",
    "        labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "        return labels_one_hot\n",
    "    elif nlevels == 2:\n",
    "        # assume that labels_dense has same column length\n",
    "        num_labels = labels_dense.shape[0]\n",
    "        num_length = labels_dense.shape[1]\n",
    "        labels_one_hot = np.zeros((num_labels, num_length, num_classes), dtype=np.int32)\n",
    "        layer_idx = np.arange(num_labels).reshape(num_labels, 1)\n",
    "        # this index selects each component separately\n",
    "        component_idx = np.tile(np.arange(num_length), (num_labels, 1))\n",
    "        # then we use `a` to select indices according to category label\n",
    "        labels_one_hot[layer_idx, component_idx, labels_dense] = 1\n",
    "        return labels_one_hot\n",
    "    else:\n",
    "        raise ValueError('nlevels can take 1 or 2, not take {}.'.format(nlevels))\n",
    "\n",
    "\n",
    "def prepare_preprocessor(X, y, use_char=True):\n",
    "    p = Preprocessor()\n",
    "    p.fit(X, y)\n",
    "\n",
    "    return p\n",
    "\n",
    "p = prepare_preprocessor(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前処理の関数を定義できたので、次にデータ生成部分の処理を描いてあげます。\n",
    "これは、バッチごとに前処理器を用いてデータを生成する処理になります。\n",
    "以下のように定義できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(data, labels, batch_size, shuffle=False, preprocessor=None):\n",
    "    num_batches_per_epoch = int((len(data) - 1) / batch_size) + 1\n",
    "\n",
    "    def data_generator():\n",
    "        \"\"\"\n",
    "        Generates a batch iterator for a dataset.\n",
    "        \"\"\"\n",
    "        data_size = len(data)\n",
    "        while True:\n",
    "            # Shuffle the data at each epoch\n",
    "            if shuffle:\n",
    "                shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "                shuffled_data = data[shuffle_indices]\n",
    "                shuffled_labels = labels[shuffle_indices]\n",
    "            else:\n",
    "                shuffled_data = data\n",
    "                shuffled_labels = labels\n",
    "\n",
    "            for batch_num in range(num_batches_per_epoch):\n",
    "                start_index = batch_num * batch_size\n",
    "                end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "                X, y = shuffled_data[start_index: end_index], shuffled_labels[start_index: end_index]\n",
    "                if preprocessor:\n",
    "                    yield preprocessor.transform(X, y)\n",
    "                else:\n",
    "                    yield X, y\n",
    "\n",
    "    return num_batches_per_epoch, data_generator()\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_steps, train_batches = batch_iter(\n",
    "    x_train, y_train, BATCH_SIZE, preprocessor=p)\n",
    "valid_steps, valid_batches = batch_iter(\n",
    "    x_valid, y_valid, BATCH_SIZE, preprocessor=p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ではモデルを定義しましょう。フレームワークにはKerasを使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, LSTM, Bidirectional, Embedding, Input, Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "def build_model(vocab_size, ntags, embedding_size=100, n_lstm_units=100, dropout=0.5):\n",
    "    sequence_lengths = Input(batch_shape=(None, 1), dtype='int32')\n",
    "    word_ids = Input(batch_shape=(None, None), dtype='int32')\n",
    "    word_embeddings = Embedding(input_dim=vocab_size,\n",
    "                                output_dim=embedding_size,\n",
    "                                mask_zero=True)(word_ids)\n",
    "    x = Dropout(dropout)(word_embeddings)\n",
    "\n",
    "    x = Bidirectional(LSTM(units=n_lstm_units, return_sequences=True))(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(n_lstm_units, activation='tanh')(x)\n",
    "    pred = Dense(ntags, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=[word_ids, sequence_lengths], outputs=[pred])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model(p.vocab_size(), p.tag_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上で学習の準備が整いました。実際に学習させてみましょう。\n",
    "最適化アルゴリズムには`Adam`を使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "180/180 [==============================] - 2133s - loss: 0.8308 - acc: 0.8659 - val_loss: 0.7094 - val_acc: 0.8651\n",
      "Epoch 2/5\n",
      "180/180 [==============================] - 2140s - loss: 0.6521 - acc: 0.8745 - val_loss: 0.5965 - val_acc: 0.8857\n",
      "Epoch 3/5\n",
      "180/180 [==============================] - 2266s - loss: 0.5268 - acc: 0.8967 - val_loss: 0.4909 - val_acc: 0.9056\n",
      "Epoch 4/5\n",
      "180/180 [==============================] - 2257s - loss: 0.4398 - acc: 0.9117 - val_loss: 0.4255 - val_acc: 0.9170\n",
      "Epoch 5/5\n",
      "180/180 [==============================] - 2076s - loss: 0.3856 - acc: 0.9202 - val_loss: 0.3832 - val_acc: 0.9228\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x148bcfa90>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "MAX_EPOCH = 5\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                          optimizer=Adam(),\n",
    "                          metrics=['acc'],\n",
    "                         )\n",
    "\n",
    "model.fit_generator(generator=train_batches,\n",
    "                    steps_per_epoch=train_steps,\n",
    "                    validation_data=valid_batches,\n",
    "                    validation_steps=valid_steps,\n",
    "                    epochs=MAX_EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}